{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-09T07:33:30.744196800Z",
     "start_time": "2025-02-09T07:33:30.736166Z"
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c98bb9ace7448d37",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T07:33:30.768058400Z",
     "start_time": "2025-02-09T07:33:30.749554400Z"
    }
   },
   "outputs": [],
   "source": [
    "class Monte_Calro_Method:\n",
    "    def __init__(self, bin_list, num_episodes, gamma, epsilon, max_steps):\n",
    "        self.bin_list = bin_list\n",
    "        self.num_episodes = num_episodes\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.max_steps = max_steps\n",
    "\n",
    "    def discretize(self, obs):\n",
    "        idxs = [\n",
    "            min(max(np.digitize(o, b) - 1, 0), len(b) - 1)\n",
    "            for o, b in zip(obs, self.bin_list)\n",
    "        ]\n",
    "        return tuple(idxs)\n",
    "\n",
    "    def decay_epsilon(self, rewards_log):\n",
    "        if len(rewards_log) == 10:\n",
    "            self.best_reward = np.mean(rewards_log[-10:])\n",
    "\n",
    "        elif len(rewards_log) > 10 and len(rewards_log) % 100 == 0:\n",
    "            avg_reward = np.mean(rewards_log[-10:])  # 최근 10개 보상 평균\n",
    "            if avg_reward > self.best_reward:  # 최고 보상을 갱신하면 감소\n",
    "                self.best_reward = avg_reward\n",
    "                self.epsilon = max(0, self.epsilon * 0.999)\n",
    "\n",
    "    def fit(self, env, Q, count):\n",
    "        rewards_log = []\n",
    "        for _ in tqdm(range(self.num_episodes)):\n",
    "            # (a) 에피소드 생성\n",
    "            obs, info = env.reset()\n",
    "            states, actions, rewards = [], [], []\n",
    "\n",
    "            self.decay_epsilon(rewards_log)\n",
    "\n",
    "            for _step in range(self.max_steps):\n",
    "                s = self.discretize(obs)\n",
    "                # ε-탐욕\n",
    "                if random.random() < self.epsilon:\n",
    "                    a = random.randint(0, 1)\n",
    "                else:\n",
    "                    a = np.argmax(Q[s])\n",
    "\n",
    "                obs_next, r, done, truncated, info = env.step(a)\n",
    "\n",
    "                states.append(s)\n",
    "                actions.append(a)\n",
    "                rewards.append(r)\n",
    "\n",
    "                obs = obs_next\n",
    "                if done or truncated:\n",
    "                    break\n",
    "\n",
    "            # (b) 에피소드별 총 보상\n",
    "            ep_reward = np.sum(rewards)\n",
    "            rewards_log.append(ep_reward)\n",
    "\n",
    "            # (c) 역방향 누적합으로 G 계산\n",
    "            #  예: G[t] = r[t] + gamma*r[t+1] + ... (역순으로 누적)\n",
    "            # vector화: reversed cumsum\n",
    "            #   r = [r0, r1, ..., rT-1],  G[T-1] = r[T-1], G[t] = r[t] + gamma*G[t+1]\n",
    "            R = np.array(rewards, dtype=np.float32)\n",
    "            T = len(R)\n",
    "            G = np.zeros(T, dtype=np.float32)\n",
    "            G[-1] = R[-1]\n",
    "            for i in range(T - 2, -1, -1):\n",
    "                G[i] = R[i] + self.gamma * G[i + 1]\n",
    "\n",
    "            # (d) Q[s][a]를 “점진적 평균”으로 업데이트\n",
    "            #     count[s][a] += 1\n",
    "            #     Q[s][a] <- Q[s][a] + (G - Q[s][a]) / count[s][a]\n",
    "            #   한 에피소드에서 같은 (s,a)가 여러 번 나올 수 있으니, 모두 처리\n",
    "            #   (Every-Visit MC)\n",
    "            #   벡터화(ish) 처리: 그래도 (s,a)별 딕셔너리 접근은 loop 필요\n",
    "            for i in range(T):\n",
    "                s_i, a_i = states[i], actions[i]\n",
    "                g_i = G[i]\n",
    "                count[s_i][a_i] += 1\n",
    "                c = count[s_i][a_i]\n",
    "                # Q <- Q + (g - Q)/c\n",
    "                Q[s_i][a_i] += (g_i - Q[s_i][a_i]) / c\n",
    "\n",
    "        env.close()\n",
    "        return Q, rewards_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eac759414a169f04",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T07:33:30.768058400Z",
     "start_time": "2025-02-09T07:33:30.756121500Z"
    }
   },
   "outputs": [],
   "source": [
    "# 1) 환경 & 이산화 준비\n",
    "env = gym.make('CartPole-v1')\n",
    "num_bins = 8\n",
    "# 관측 범위\n",
    "ranges = [(-4.8, 4.8), (-3, 3), (-0.418, 0.418), (-4, 4)]\n",
    "bins_list = [np.linspace(lo, hi, num_bins) for (lo, hi) in ranges]\n",
    "\n",
    "# 2) Q, 카운트(점진적 평균)\n",
    "Q = defaultdict(lambda: np.zeros(env.action_space.n, dtype=np.float32))\n",
    "count = defaultdict(lambda: np.zeros(env.action_space.n, dtype=np.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b007c4ab8086cd5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T07:34:11.396320700Z",
     "start_time": "2025-02-09T07:33:30.768058400Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 6475/50000 [00:40<04:31, 160.48it/s]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MC = Monte_Calro_Method(bins_list, num_episodes=50000, gamma=0.99, epsilon=0.05, max_steps=500)\n",
    "Q, rewards = MC.fit(env, Q, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7529438db71fe446",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-02-09T07:34:11.392318700Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 3), dpi=400)\n",
    "plt.plot(np.array(rewards), label=\"Monte Carlo Method\")\n",
    "plt.title('CartPole-v1')\n",
    "plt.xlabel('Episode', fontsize=15)\n",
    "plt.ylabel('Reward', fontsize=15)\n",
    "plt.legend(loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"초반 10개 에피소드 보상:\", rewards[:10])\n",
    "print(\"마지막 10개 에피소드 보상:\", rewards[-10:])\n",
    "avg_reward_recent = np.mean(rewards[-100:])\n",
    "print(f\"최근 100 에피소드 평균 보상: {avg_reward_recent:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
